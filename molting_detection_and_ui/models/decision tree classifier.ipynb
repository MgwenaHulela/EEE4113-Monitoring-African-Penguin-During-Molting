import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# =============================================
# 1. Enhanced Dataset Generation with Noise/Outliers
# =============================================
def generate_noisy_molt_data(n_penguins=150, noise_level=0.1, outlier_prob=0.02):
    np.random.seed(42)
    penguin_ids = [f"P{str(i).zfill(3)}" for i in range(1, n_penguins+1)]
    sexes = np.random.choice(['Male', 'Female'], n_penguins)
    base_weights = np.clip(np.random.normal(3.5, 0.4, n_penguins), 2.7, 4.3)

    data = []
    for pid, sex, bw in zip(penguin_ids, sexes, base_weights):
        # Molt cycle parameters
        pre_days = np.random.randint(14, 21)
        mid_days = np.random.randint(28, 42)
        post_days = np.random.randint(21, 35)
        total_days = pre_days + mid_days + post_days

        # Generate 1-2 outliers per penguin
        outlier_days = np.random.choice(total_days, size=np.random.randint(1, 3), replace=False)

        for day in range(total_days):
            # Base patterns
            if day < pre_days:
                stage = "Pre-molt"
                base_weight = bw * (1 + 0.01*day)
                base_activity = np.random.normal(7, 1)
            elif day < pre_days + mid_days:
                stage = "Mid-molt"
                base_weight = bw * (0.9 - 0.005*(day-pre_days))
                base_activity = np.random.normal(3, 1)
            else:
                stage = "Post-molt"
                base_weight = bw * (0.85 + 0.003*(day-pre_days-mid_days))
                base_activity = np.random.normal(5, 1)

            # Add noise
            weight = base_weight * (1 + np.random.normal(0, noise_level/3))
            activity = base_activity * (1 + np.random.normal(0, noise_level))
            food_intake = np.random.normal(0.7 if stage == "Mid-molt" else 0.9, 0.1)

            # Inject outliers (5% probability)
            if day in outlier_days or np.random.random() < outlier_prob:
                outlier_type = np.random.choice(['weight', 'activity', 'stage', 'combined'])

                if outlier_type == 'weight':
                    weight *= np.random.choice([0.7, 1.5])  # Extreme low/high
                elif outlier_type == 'activity':
                    activity = np.random.choice([1, 10])  # Min/Max activity
                elif outlier_type == 'stage':
                    stage = np.random.choice(['Pre-molt', 'Mid-molt', 'Post-molt'])
                else:  # combined
                    weight *= 1.8
                    activity = 0.5
                    stage = 'Mid-molt' if stage != 'Mid-molt' else 'Post-molt'

                # Tag outliers for analysis
                is_outlier = True
            else:
                is_outlier = False

            data.append([
                pid,
                sex,
                day,
                round(weight, 2),
                stage,
                round(activity, 1),
                round(food_intake, 2),
                is_outlier
            ])

    df = pd.DataFrame(data, columns=[
        'Penguin_ID', 'Sex', 'Day_in_Cycle',
        'Weight_kg', 'Molt_Stage', 'Activity',
        'Food_Intake', 'Is_Outlier'
    ])
    return df

# Generate noisy dataset
noisy_molt_df = generate_noisy_molt_data()
print(f"Generated {len(noisy_molt_df)} records")
print(f"Outliers detected: {noisy_molt_df['Is_Outlier'].sum()}")

# =============================================
# 2. Robust Feature Engineering
# =============================================
# Visualize outliers
plt.figure(figsize=(12, 5))
plt.scatter(
    noisy_molt_df['Day_in_Cycle'],
    noisy_molt_df['Weight_kg'],
    c=noisy_molt_df['Is_Outlier'].map({True: 'red', False: 'blue'}),
    alpha=0.6
)
plt.title("Weight Measurements with Outliers (Red)")
plt.xlabel("Day in Cycle")
plt.ylabel("Weight (kg)")
plt.savefig('outliers_visualization.png')
plt.show()

# Handle outliers (option 1: keep but mark, option 2: winsorize)
# Here we'll keep them but add outlier flag as a feature
noisy_molt_df['Sex_Code'] = LabelEncoder().fit_transform(noisy_molt_df['Sex'])
noisy_molt_df['Stage_Code'] = LabelEncoder().fit_transform(noisy_molt_df['Molt_Stage'])
noisy_molt_df['Outlier_Flag'] = noisy_molt_df['Is_Outlier'].astype(int)

# Normalize features
scaler = StandardScaler()
features = scaler.fit_transform(
    noisy_molt_df[['Weight_kg', 'Activity', 'Food_Intake', 'Day_in_Cycle', 'Sex_Code', 'Outlier_Flag']]
)
labels = noisy_molt_df['Stage_Code']

# =============================================
# 3. Noise-Resistant Model Training
# =============================================
def build_robust_model(input_shape, n_classes):
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_shape=input_shape),
        layers.Dropout(0.4),
        layers.BatchNormalization(),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(n_classes, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    features, labels,
    test_size=0.2,
    stratify=labels,
    random_state=42
)

# Train with early stopping
model = build_robust_model((X_train.shape[1],), 3)
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

# =============================================
# 4. Enhanced Evaluation
# =============================================
# Plot training history
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.legend()
plt.tight_layout()
plt.savefig('noisy_training_history.png')
plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_acc:.3f}")

# Confusion matrix with outlier analysis
y_pred = np.argmax(model.predict(X_test), axis=1)
test_df = noisy_molt_df.iloc[y_test.index].copy()
test_df['Predicted'] = y_pred

# Accuracy breakdown
print("\nAccuracy by Outlier Status:")
print(test_df.groupby('Is_Outlier').apply(
    lambda x: np.mean(x['Stage_Code'] == x['Predicted'])
))

# =============================================
# 5. Save Artifacts
# =============================================
model.save('noise_resistant_molt_model.h5')
noisy_molt_df.to_csv('noisy_penguin_dataset.csv', index=False)

print("\nâœ… Final Artifacts:")
print("- noisy_penguin_dataset.csv")
print("- noise_resistant_molt_model.h5")
print("- outliers_visualization.png")
print("- noisy_training_history.png")
